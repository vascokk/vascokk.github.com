<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: CUDA | Reinventing The Wheel]]></title>
  <link href="http://vascokk.github.com/blog/categories/cuda/atom.xml" rel="self"/>
  <link href="http://vascokk.github.com/"/>
  <updated>2013-03-23T22:27:29+00:00</updated>
  <id>http://vascokk.github.com/</id>
  <author>
    <name><![CDATA[Vasco Kolarov]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Machine Learning in Erlang and CUDA]]></title>
    <link href="http://vascokk.github.com/blog/2013/03/23/machine-learning-in-erlang-and-cuda/"/>
    <updated>2013-03-23T18:49:00+00:00</updated>
    <id>http://vascokk.github.com/blog/2013/03/23/machine-learning-in-erlang-and-cuda</id>
    <content type="html"><![CDATA[<p>Since I finished the "Machine Learning" course on Coursera, I wanted to try it in Erlang, Unfortunately Erlang is notorious with its inability to do intensive math calculations. In short - no matrix calculations - no Machine learning. Solution - use C and call C-functions from Erlang. Or better - use the GPU (in C) and call the functions from Erlang. I guess, this approach is no more qualified as "pure Erlang" one, but that is the best we can get. In fact - most of the Python ML libraries, so famous in the ML society, are using NumPy library, which guess what - is written in C (at least - the important part of it).</p>

<p>So, first thing to do was - google "erlang matrix operations cuda gpu"....Almost nothing. Actually, I found two projects - OpenCL bindings for erlang <a href="https://github.com/tonyrog/cl">https://github.com/tonyrog/cl</a> and Kevin Smith's <a href="https://github.com/kevsmith/pteracuda">Pteracuda</a>. I liked the Kevin's approach, as using the Thrust library would hide the pure C boilerplate CUDA code. Also some algorithms are one-liners implemented with Thrust.</p>

<p>I forked Pteracuda and added matrix operations (CUBLAS-based), but since it had some parts that I didn't need (like: strings, search, min/max), I decided to create another project and so - the <a href="https://github.com/vascokk/NumEr">NumEr</a> was born :)</p>

<p>Basically, it is a bunch of Erlang NIF functions for BLAS operations on vectors and matrices. Both are natively implemented as Thrust host/device vectors and special "buffer" classes are used to transfer them from Erlang to CUDA and back.</p>

<p>Here is an example:</p>

<p>``` erlang
% this is a row-major matrix:
A = [[4.0,6.0,8.0,2.0],[5.0,7.0,9.0,3.0]].</p>

<p>%this is a vector:
X = [2.0,5.0,1.0,7.0].</p>

<p>% create a CUDA context and transfer to "buffers"
{ok, Ctx} = numer_nifs:new_context().
{ok, Buf_A} = numer_nifs:new_matrix_float_buffer(Ctx, A, ?ROW_MAJOR).
{ok, Buf_X} = numer_nifs:new_float_buffer(Ctx).
numer_nifs:write_buffer(Buf_X, X).
```</p>

<p>As you see one of the parameters in the matrix buffer is "?ROW_MAJOR". It is kinda borrowed from Boost library, but not yet fully implemented in NumEr. I mean - currently only row-major matrices are supported. However, under the hood in the Thrust vectors the numbers are stored in column-major format. I chose to do it in this way, because the CUBLAS library is using column-major storage - being a derivative of the FORTRAN BLAS library.</p>

<p>There are several modules, which are wrappers of the NIF functions like: numer_blas.erl - for BLAS operations, numer_buffer.erl - for operations on buffers (new, delete, read, write), etc.</p>

<p>Using numer_buffer module, the above example will look like:</p>

<p><code>erlang
 {ok, Ctx} = numer_context:new().
 {ok, Buf_A} = numer_buffer:new(Ctx, matrix, float, row_major, A).
 {ok, Buf_X} = numer_buffer:new(Ctx, float).
 numer_buffer:write(Buf_X, X).
</code></p>

<p>Now, the more interesting part - calculations with vectors and matrices. Here is the BLAS GEMV example:</p>

<p>``` erlang
%  GEMV: y <- α op ( A ) x + β y
gemv_test()-></p>

<pre><code>{ok, Ctx} = numer_context:new(),
A = [[4.0,6.0,8.0,2.0],[5.0,7.0,9.0,3.0]],
_m = 2, %rows A
_n = 4, %columns A
_alpha = 1.0,
_beta = 0.0,
X = [2.0,5.0,1.0,7.0],
Y = [0.0, 0.0], 
{ok, Buf_A} = numer_buffer:new(Ctx, matrix, float, row_major, A),
{ok, Buf_X} = numer_buffer:new(Ctx, float),
numer_buffer:write(Buf_X, X),
{ok, Buf_Y} = numer_buffer:new(Ctx, float),
numer_buffer:write(Buf_Y, Y),
ok = numer_blas:gemv(Ctx, no_transpose , _m, _n, _alpha, Buf_A, Buf_X, _beta, Buf_Y),
{ok, [60.0,75.0]} = numer_buffer:read(Buf_Y),
ok = numer_buffer:destroy(Buf_A),
ok = numer_buffer:destroy(Buf_X),
ok = numer_buffer:destroy(Buf_Y),
ok = numer_context:destroy(Ctx).
</code></pre>

<p>```</p>

<p>As for the ML part of the whole exercise - there is an implementation of the Logistic Regression (without regularization) algorithm. Take a look at the numer_logreg.erl module.</p>

<p>The numer_ml.erl module contains an all-native implementation of Logistic Regression and numer_logreg.erl - implementation using buffers and NIFs. The first one I used to compare the speed between native and "Erlang" implementations.</p>

<p>Since using buffer operation can make the implementation awkward, there is also a helper module - numer_helpers.erl, wich can be used for prototyping the algorithms. WARNING - it is extremely slow and suitable ONLY for prototyping. Here is how you can use it:</p>

<p>``` erlang
gemv_2_test()-></p>

<pre><code>A = [[4.0,6.0,8.0,2.0],[5.0,7.0,9.0,3.0]],
_alpha = 1.0,
_beta = 0.0,
X = [2.0,5.0,1.0,7.0],
Res = numer_helpers:gemv(no_transpose , _alpha, A, X, _beta, []),
?assertEqual([60.0,75.0], Res).
</code></pre>

<p>```</p>

<p>It is much more readable and useful for one-time calculations, but in the ML "training" stage (with hundreds of iterations) it will be unusable, due to the multiple buffer creations and transfers.</p>

<p>The project is still a work in progress and needs a lot of polishing. So, if anyone is willing to give a hand I'll be more than happy. Any suggestions to improve the framework are also very welcome.</p>

<p>That's all folks! Happy hacking! :-)</p>
]]></content>
  </entry>
  
</feed>
