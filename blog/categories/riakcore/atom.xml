<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: RiakCore | Reinventing The Wheel]]></title>
  <link href="http://vascokk.github.com/blog/categories/riakcore/atom.xml" rel="self"/>
  <link href="http://vascokk.github.com/"/>
  <updated>2013-03-23T22:53:34+00:00</updated>
  <id>http://vascokk.github.com/</id>
  <author>
    <name><![CDATA[Vasco Kolarov]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Erlang Real Time Server - Part 3 - Putting Parts Together]]></title>
    <link href="http://vascokk.github.com/blog/2012/07/15/erlang-real-time-server-part-3-putting-them-together/"/>
    <updated>2012-07-15T17:16:00+01:00</updated>
    <id>http://vascokk.github.com/blog/2012/07/15/erlang-real-time-server-part-3-putting-them-together</id>
    <content type="html"><![CDATA[<h2>The need for load-balancing</h2>

<p>As you have probably noticed in <a href="http://vas.io/blog/2012/06/06/erlang-real-time-server-part-2-aggregation/">Part 2</a> we always send the Riak commands to one and the same node. That doesn't seem to be right, so, what about some load balanced distibution of the commands among the cluster nodes? There are at least two ways to solve this:</p>

<ul>
<li>send commands from Diameter Server to Riak cluster in load-balanced fashion, or:</li>
<li>couple Diameter server with  RiakCore application in one node and send Diameter messages directly to the RiakCluster in a load-balanced fashion.</li>
</ul>


<p>If we choose the first approach, we can use HTTP or ProtocolBuffer interface to access Riak from <em>diaserver</em> and use some HTTP/TCP proxy to load-balance the requests (e.g. HAProxy is a good one). The inconvenience here comes from the fact that we have to write a separate HTTP POST requests or implement a PB interface for every Diameter command. (But, this is probably a good idea for a future blog post)</p>

<p>In the second case we can use a Diameter Relay/Forward agent to distribute Diameter messages among the cluster or (again) a TCP proxy, since one of the possible Diameter transports is TCP. For the sake of simplicity I'll use this approach.</p>

<p>First of all, as our applications will be running on the same Riak cluster, I'll change the <em>diaserver</em> callback module. At the moment this module is responsible for creating the Diameter Accounting Answer message. Instead, I'll just make a call to aggregation:accounting() function with parameter - the Diameter Accounting Request.</p>

<p>Change the <em>accounting()</em> function in <em>aggregation.erl</em> module:</p>

<p>``` erlang
accounting(Req) -></p>

<pre><code>DocIdx = riak_core_util:chash_key({&lt;&lt;"accounting"&gt;&gt;, term_to_binary(now())}),
PrefList = riak_core_apl:get_primary_apl(DocIdx, 1, aggregation),
[{IndexNode, _Type}] = PrefList,
riak_core_vnode_master:sync_spawn_command(IndexNode, {accounting, Req}, aggregation_vnode_master).
</code></pre>

<p>```</p>

<p>Don't forget to change also the arity in the export clause to /1.</p>

<p>Next, the handle_call() in the aggregation_vnode module:</p>

<p>``` erlang
handle_command({accounting, Req}, _Sender, State) -></p>

<pre><code>%%TODO Process accounting request
Req,
ResponseCode = ?'DIAMETER_BASE_RESULT-CODE_DIAMETER_SUCCESS',

{reply, ResponseCode, State};
</code></pre>

<p>```</p>

<p>For the sake of the exercise, we are just going to return SUCCESS, but actually this is the place where you should do all the processing of the Diameter request.</p>

<p>The last step is to call aggregation:accounting(Req) from the diameter callback module:</p>

<p>``` erlang server_cb.erl
handle_request(#diameter_packet{msg = Req, errors = []}, <em>SvcName, {</em>, Caps})</p>

<pre><code>              when is_record(Req, diameter_base_ACR) -&gt;

    #diameter_caps{origin_host = {OH,_},
               origin_realm = {OR,_}}
        = Caps,

        #diameter_base_ACR{'Session-Id' = Id,
                   'Accounting-Record-Type' = RecType,
                           'Accounting-Record-Number' = RecNum,
                       'Acct-Application-Id' = AccAppId }
        = Req,

    ResultCode = aggregation:accounting(Req),

    Ans = #diameter_base_ACA{'Result-Code' = ResultCode, %%?'DIAMETER_BASE_RESULT-CODE_DIAMETER_SUCCESS',
                   'Origin-Host' = OH,
                       'Origin-Realm' = OR,
                       'Session-Id' = Id,
                           'Accounting-Record-Type' = RecType,
                           'Accounting-Record-Number' = RecNum,
                       'Acct-Application-Id' = AccAppId
    },

    {reply, Ans}.
</code></pre>

<p>```</p>

<p>Now, what is left is - to put <em>diaserver</em> and <em>aggregation</em> applications together in one erlang "release":</p>

<h2>Building the "RTS" node</h2>

<p>I'm going to the same as I did in the previous post - using the rebar-riak-core templates, but this time I'll use a slightly modified <a href="https://github.com/vascokk/rebar_riak_core/blob/master/riak_core_multinode_simple.template">"multinode template"</a>:</p>

<p>``` bash
$ cd rel
$ mkdir rts
$ cd rts
$ ../../rebar -f create template=riak_core_multinode_simple nodeid=rts target_dir=.</p>

<p>==> rts (create)
Writing ./reltool.config
Writing ./files/erl
Writing ./files/nodetool
Writing ./files/rts
Writing ./files/app.config
Writing ./files/vm.args
Writing .gitignore
Writing Makefile
Writing ./files/rts-admin
Writing ./vars.config
Writing ./vars/dev1.config
Writing ./vars/dev2.config
Writing ./vars/dev3.config</p>

<p>```</p>

<p>Now, we have to edit <em>reltool.config</em> and <em>app.config</em>:</p>

<p>For <em>reltool.config</em> we need to:</p>

<ul>
<li>change lib_dirs with appropriate paths</li>
<li>add <em>diaserver</em> and <em>aggregation</em> to the list of application need to be started by this release</li>
<li>add application specific configuration</li>
</ul>


<p>``` bash
{sys, [</p>

<pre><code>   {lib_dirs, ["../../apps/", "../../deps/"]},
   {rel, "rts", "1",
    [
     kernel,
     stdlib,
     sasl,
     %% add our applications
     aggregation,
     diaserver
    ]},
   {rel, "start_clean", "",
    [
     kernel,
     stdlib
    ]},
   {boot_rel, "rts"},
   {profile, embedded},
   {excl_sys_filters, ["^bin/.*",
                       "^erts.*/bin/(dialyzer|typer)"]},
   {app, sasl, [{incl_cond, include}]},
   %%add application specific configurations
   {app, aggregation, [{incl_cond, include}]},
   {app, diaserver, [{incl_cond, include}]}
  ]}.
</code></pre>

<p>{target_dir, "rts"}.</p>

<p>{overlay_vars, "vars.config"}.</p>

<p>{overlay, [</p>

<pre><code>       {mkdir, "data/ring"},
       {mkdir, "log/sasl"},
       {copy, "files/erl", "\{\{erts_vsn\}\}/bin/erl"},
       {copy, "files/nodetool", "\{\{erts_vsn\}\}/bin/nodetool"},
       {template, "files/app.config", "etc/app.config"},
       {template, "files/vm.args", "etc/vm.args"},
       {template, "files/rts", "bin/rts"},
       {template, "files/rts-admin", "bin/rts-admin"}
       ]}.
</code></pre>

<p>```</p>

<p>For the <em>app.config</em> we need to configure the TCP port on which diaserver will listen.</p>

<p>{% gist 3146816 %}</p>

<p>I only added the last <em>diaserver</em> tuple. As you see the {{diameter_port}} is a template parameter. It will be substituted with the value specific for each "dev" node. For this purpose we have 3 "dev" configuration files in the <em>dev</em> directory. Here is the content of the first one:</p>

<p>``` bash
%%
%% etc/app.config
%%
{ring_state_dir,        "data/ring"}.
{web_ip,                "127.0.0.1"}.
{web_port,              "8091"}.
{handoff_port,          "8101"}.
{diameter_port,         "7071"}.</p>

<p>%%
%% etc/vm.args
%%
{node,                  "rts1@127.0.0.1"}.
{cookie,                "rts"}.
```</p>

<p>In this case I put 7071 for Diameter port, for the other 2 nodes the ports will be 7072 and 7073. Using this "dev" configurations will allow us to run 3 (or more if we want) Riak nodes running both <em>aggregation</em> and <em>diaserver</em> applications on the same host.</p>

<p>Compile and build the development release:</p>

<p><code>bash
&lt;project_root&gt;$ make
&lt;project_root&gt;$ make devrel
==&gt; rts (generate)
</code></p>

<p>Last thing - we need the  <em>rel/rts/files/rts-admin</em> script to join our Riak nodes once they are started. If you open this file you'll notice that it calls "myapp_console" application, we have to change this to "aggregation_console" (see the <em>app/aggregation/src</em> directory).</p>

<p>And we are done with the RTS!</p>

<p>We can now run 3 nodes, each one listening for Diameter messages on a different port. As you remember, the initial idea was to distribute the messages to these nodes via TCP proxy, so let's:</p>

<h2>Setup the HAProxy</h2>

<p>This part is relatively easy. First download and install the HAProxy:</p>

<p>``` bash
$ wget http://haproxy.1wt.eu/download/1.4/src/haproxy-1.4.20.tar.gz</p>

<p>.....</p>

<p>$ tar -xzf haproxy-1.4.20.tar.gz</p>

<p>.....</p>

<p>$ cd haproxy-1.4.20
$ make TARGET=linux26
$ sudo make install
```</p>

<p>The above <em>make</em> has the option <em>TARGET=linux26</em>, which might be different in your case. To determine the linux kernel version use "uname -r" command.</p>

<p>If the HAProxy installation is okay, create a configuration file. In our case it looks like this:</p>

<p>``` bash dev.haproxy.config
global
  # specify the maximum connections across the board
  maxconn 2048
  # enable debug output
  debug</p>

<h1>now set the default settings for each sub-section</h1>

<p>defaults
  # stick with http traffic
  mode http
  # set the number of times HAProxy should attempt to
  # connect to the target
  retries 3
  # specify the number of connections per front and
  # back end
  maxconn 1024
  # specify some timeouts (all in milliseconds)
  timeout connect 5000</p>

<h6>##### Riak Configuration</h6>

<p>frontend diaserver
  mode tcp</p>

<p>  # bind to default DIAMETER port 3868
  bind 127.0.0.1:3868</p>

<p>  # Default to the riak cluster configuration
  default_backend riak_cluster</p>

<p>  # timeouts
  timeout client 1200000</p>

<h1>Here is the magic bit which load balances across</h1>

<h1>our instances of riak_core which are clustered</h1>

<h1>together</h1>

<p>backend riak_cluster
  mode tcp
  balance roundrobin
  # timeouts
  timeout server 1200000
  timeout connect 3000</p>

<p>  server rts1 127.0.0.1:7071 check
  server rts2 127.0.0.1:7072 check
  server rts3 127.0.0.1:7073 check</p>

<p>```</p>

<p>The idea here is pretty obvious (right?): our "frontend" (i.e. what the external world will see) is the standard Diameter port 3868. All the TCP packets sent to this port will be forwarded on a round-robin fashion to the "backend", which is our 3 dev release erlang nodes, each of them listening to a different diameter port.</p>

<p>And that's it! We only need to run everything and see how it works:</p>

<p>Open a terminal window and start the first node:</p>

<p>``` bash
$ ./rel/rts/dev/dev1/bin/rts console</p>

<p>19:01:11.204 [info] Application crypto started on node 'rts1@127.0.0.1'
19:01:11.208 [info] Application riak_sysmon started on node 'rts1@127.0.0.1'
19:01:11.239 [info] Application inets started on node 'rts1@127.0.0.1'
19:01:11.243 [info] Application mochiweb started on node 'rts1@127.0.0.1'
** Found 0 name clashes in code paths
19:01:11.277 [info] Application webmachine started on node 'rts1@127.0.0.1'
19:01:11.311 [info] Application os_mon started on node 'rts1@127.0.0.1'
19:01:11.322 [info] Application folsom started on node 'rts1@127.0.0.1'
19:01:11.470 [info] New capability: {riak_core,vnode_routing} = proxy
19:01:11.471 [info] New capability: {riak_core,staged_joins} = true
19:01:11.473 [info] Application riak_core started on node 'rts1@127.0.0.1'
19:01:11.509 [info] Waiting for application aggregation to start (0 seconds).
19:01:11.515 [info] Application aggregation started on node 'rts1@127.0.0.1'
19:01:11.568 [info] Application diameter started on node 'rts1@127.0.0.1'
19:01:11.575 [info] Starting diameter on IP: {127,0,0,1}
19:01:11.575 [info] Starting diameter on port: 7071
19:01:11.611 [info] Wait complete for application aggregation (0 seconds)
19:01:11.614 [info] Application diaserver started on node 'rts1@127.0.0.1'
Eshell V5.9.1  (abort with ^G)
(rts1@127.0.0.1)1>
```</p>

<p>Open a new terminal and start the second node and join it to the first one::</p>

<p>``` bash
$ rel/rts/dev/dev2/bin/rts start
$ rel/rts/dev/dev2/bin/rts-admin join rts1@127.0.0.1
Sent join request to rts1@127.0.0.1</p>

<p>....on the first one you should get:</p>

<p>(rts1@127.0.0.1)1> 20:13:44.622 [info] 'rts2@127.0.0.1' joined cluster with status 'valid'</p>

<p>....attach to the second rts console:</p>

<p>$ rel/rts/dev/dev2/bin/rts attach</p>

<p>```</p>

<p>Now, run the HAProxy:</p>

<p>``` bash
 $ sudo haproxy -f dev.haproxy.config -d
Available polling systems :</p>

<pre><code> sepoll : pref=400,  test result OK
  epoll : pref=300,  test result OK
   poll : pref=200,  test result OK
 select : pref=150,  test result OK
</code></pre>

<p>Total: 4 (4 usable), will use sepoll.
Using sepoll() as the polling mechanism.
[WARNING] 200/202744 (10525) : Server riak_cluster/rts3 is DOWN, reason: Layer4 connection problem, info: "Connection refused", check duration: 0ms. 2 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.</p>

<p>```</p>

<p>Run another erlang shell and test with diameter client (see Part 2):</p>

<p>``` bash
Eshell V5.9.1  (abort with ^G)
1> diameter:start().
ok
2> client:start().
ok
3> client:connect(tcp).
{ok,#Ref&lt;0.0.0.50>}
4> client:call_ACR().
{ok,{diameter_base_ACA,"client;1404231385;1;nonode@nohost",</p>

<pre><code>                   2001,"diameter_srv.example.com","example.com",1,1,[],
                   [],[],[],[],[],[],[],[],[],[],[],[]}}
</code></pre>

<p>5></p>

<p>....you should see the connection request accepted on the console running the HAProxy:</p>

<p>00000000:diaserver.accept(0004)=0005 from [127.0.0.1:33979]
```</p>

<p>If you execute the client:call_ACR() several times, you should see the request going to the first or the second node. The balance policy is not really a round-robin, because we have another routing layer in RiacCore, which sends the requests to the appropriate Riak partition.</p>

<p>You can now create a release node (execute: <em>make rel</em>), deploy the node on several AWS instances and here we go - a real-time billing in the cloud :-) Well, of course,  we are not even close to this - we need to implement all the Diameter commands and we need a backend system to manage our subscribers and tariffs, but at least, we just built some foundations for a Riak-based AAA system.</p>

<p>Project files are available one GitHub in repository <a href="https://github.com/vascokk/diameter-test">diameter_test</a>. Happy hacking! :-)</p>

<h2>Credits</h2>

<ul>
<li>OJ Reeves for the HAProxy <a href="http://buffered.io/posts/webmachine-erlydtl-and-riak-part-2/">mini-tutorial</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Erlang Real Time Server - Part 2 - Aggregation]]></title>
    <link href="http://vascokk.github.com/blog/2012/06/06/erlang-real-time-server-part-2-aggregation/"/>
    <updated>2012-06-06T18:39:00+01:00</updated>
    <id>http://vascokk.github.com/blog/2012/06/06/erlang-real-time-server-part-2-aggregation</id>
    <content type="html"><![CDATA[<h2>High level overview of the RTS</h2>

<p>In a schematic view, the RTS will look like this:</p>

<p><img src="/images/rts-part-2/RTS1.png" alt="" /></p>

<p>The idea here is: As a result of Diameter request, command on Aggregation module will be invoked (e.g. "accounting()"). As the job, usually performed by this module is computationally intensive, the Aggregation will run on a Riak cluster and the command will be executed within this cluster.</p>

<h2>Setting up a Riak cluster (or  "A very lame introduction to Riak")</h2>

<p>Setting up a cluster could be a tedious job. Fortunately, there are always some giant's shoulders you can step on*. In this case I will use the Ryan Zezeski's RiakCore rebar <a href="https://github.com/rzezeski/rebar_riak_core">templates</a></p>

<p>Once you clone the templates repository from GitHub, make a new Erlang application directory in <em>apps</em> and run <em>rebar</em> from it:</p>

<p><code>bash
&lt;project_root&gt; $ ./mkdir apps/aggregation
&lt;project_root&gt; $ cd apps/aggregation
&lt;project_root&gt;/apps/aggregation $ ../../rebar -f create template=riak_core_multinode appid=aggregation nodeid=aggregation
</code></p>

<p>With this run correctly, you should have the following files in the application's <em>src</em> directory:</p>

<p><code>bash
aggregation_app.erl
aggregation.app.src
aggregation_console.erl
aggregation.erl
aggregation.hrl
aggregation_node_event_handler.erl
aggregation_ring_event_handler.erl
aggregation_sup.erl
aggregation_vnode.erl
</code></p>

<p>What you are looking at is an application, which will start the Riak's working horse - the vnode. The vnode ("virtual node") is the building block of the Riak cluster. Riak itself is heavily influenced by Amazon's Dynamo paper - <a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">"Dynamo: Amazonâ€™s Highly Available Key-value Store"</a>. It is worth reading, but if you are not in an "academic mood" right now, here are some highlights:</p>

<ul>
<li>Riak is a distributed, highly scalable, key-value store</li>
<li>Riak is usually deployed on a cluster (set of physical hosts called "nodes")</li>
<li>each node runs a certain number of "virtual nodes" (or "vnodes")</li>
<li>the vnode is responsible for a partition of the Riak "Ring"</li>
<li>the Ring is a circular, ordered, 160 bit integer space (i.e. integers from 0 to 2<sup>160</sup> placed in circle)</li>
<li>each element of the Ring represents a hash value computed of the key from a "key-value" pair (in fact it will be computed of the "bucket"+"key", but this is not so important at the moment)</li>
<li>the number of partitions is always the same, whether you add a new node to the Cluster or take one off (i.e. a bit of a planning is needed upfront)</li>
<li>the vnode can be used to store data or to perform computations (executing commands)</li>
</ul>


<p>I'll not go further into details, as <a href="http://wiki.basho.com">wiki.basho.com</a> is your best source of information, if you want to dive in. I'll just outline the "scalability" thing and how Riak will make our life easier:</p>

<p>As you see in the second bullet from the bottom - the number of partitions is constant. It is set in the app.config's ring_creation_size parameter. Default number is 64 and there is no formal way to calculate it. It would be more or less empirically set by the developer. According to the Riak's wiki - for a mid-sized cluster of 8 to 16 nodes, the number should be between 128 and 512 partitions (always a power of 2), but this is definitely not "set on stone". Let's assume that we have set the optimal ring size already. At some point of time when the system is fully operational, we might need more computational power. Then, what we should do is:</p>

<ol>
<li>Deploy the application on a new node.</li>
<li>"join" (one-line command) the new one to a randomly chosen node from the cluster (you can really pick any member, there is no "master node").</li>
<li>....Wait...there is no 3rd... :)</li>
</ol>


<p>The new node will automatically claim a certain number of partitions, offloading the other nodes. Under the hood, Riak will use "handoff" procedures, "Gossip" protocol and other magics to do the job. What is important to us is that, due to the "consistent hashing" (more on this in some other article), the relocation of the vnodes will be transparent to the application(s) using the Riak cluster, as all the hash values are still on the same vnodes, even though, some of the vnodes are now residing on a different physical host. Cool, isn't it? The bottom line is: Scaling by adding a new host to the cluster is dead simple, but it will relocate a number of vnodes - not add new ones. So, be careful when you plan your Ring size.</p>

<p>generated rebar.conf:
``` erlang
%% -<em>- erlang -</em>-
{sub_dirs, ["rel", "apps/aggregation"]}.
{cover_enabled, true}.
{erl_opts, [debug_info, warnings_as_errors]}.
{edoc_opts, [{dir, "../../doc"}]}.
{deps, [{riak_core, "HEAD",</p>

<pre><code>     {git, "https://github.com/basho/riak_core", {"HEAD"}}}
   ]}.
</code></pre>

<p>```</p>

<p>changed rebar.conf:</p>

<p>``` erlang
%% -<em>- erlang -</em>-
{sub_dirs, ["rel", "apps/aggregation"]}.
{cover_enabled, true}.
{erl_opts, [debug_info, warnings_as_errors]}.
{edoc_opts, [{dir, "../../doc"}]}.
{deps, [{riak_core, "1.1.2",</p>

<pre><code>     {git, "https://github.com/basho/riak_core", {"1.1.2"}}}
   ]}.
</code></pre>

<p>{lib_dirs, ["apps", "deps"]}.
```</p>

<p>and compile:</p>

<p>``` bash
<project_root>/erlang-rts-part-2 $ ./rebar compile
==> lager (compile)
==> poolboy (compile)
==> protobuffs (compile)
==> basho_stats (compile)
==> riak_sysmon (compile)
==> mochiweb (compile)
==> webmachine (compile)
==> riak_core (compile)
==> rel (compile)
==> aggregation (compile)
Compiled src/aggregation_sup.erl
Compiled src/aggregation_node_event_handler.erl
Compiled src/aggregation_app.erl
Compiled src/aggregation_console.erl
Compiled src/aggregation.erl
Compiled src/aggregation_vnode.erl
Compiled src/aggregation_ring_event_handler.erl
==> erlang-rts-part-2 (compile)</p>

<p>```</p>

<h2>Create release nodes</h2>

<p>At this moment <em>rel</em> directory should contain the <em>aggregation</em> release configuration file. Because we have 2 applications (<em>diaserver</em> and <em>aggregation</em>) I'll put them in separate <em>rel</em> subdirectories.</p>

<p>First, create <em>aggregation</em> sub-directory and move the content of <em>rel</em> there. Next, edit the second line of reltool.config to match the directory structure:</p>

<p>``` bash
 {lib_dirs, ["../../apps/", "../../apps/aggregation/", "../../deps/"]},</p>

<p>```</p>

<p>Create the diaserver node (the same way as in Part1, only in different directory):</p>

<p><code>bash
mkdir diaserver
cd diaserver
rel/diaserver $ ../../rebar create-node nodeid=diaserver
==&gt; diaserver (create-node)
Writing reltool.config
Writing files/erl
Writing files/nodetool
Writing files/diaserver
Writing files/sys.config
Writing files/vm.args
Writing files/diaserver.cmd
Writing files/start_erl.cmd
</code></p>

<p>Edit reltool.config the in the way already described in Part1.</p>

<p>Create 2 aggregation release nodes. They will work on different ports and we will be able to run them on the same physical host. This way we will simulate a 2-host Riak cluster:</p>

<p>``` bash
erlang-rts-part-2/rel/aggregation $ ../../rebar generate target_dir=./dev/dev1 overlay_vars=vars/dev1.config appid=aggregation
==> aggregation (generate)
erlang-rts-part-2/rel/aggregation $ ../../rebar generate target_dir=./dev/dev2 overlay_vars=vars/dev2.config appid=aggregation
==> aggregation (generate)</p>

<p>```</p>

<p>Now, let's test the Riak cluster. Open a terminal and start the first node:</p>

<p>``` bash First console
erlang-rts-part-2/rel/aggregation $ ./dev/dev1/bin/aggregation console</p>

<p>Exec: /home/vasco/w/dia-test/erlang-rts-part-2/rel/aggregation/dev/dev1/erts-5.8.4/bin/erlexec -boot /home/vasco/w/dia-test/erlang-rts-part-2/rel/aggregation/dev/dev1/releases/1/aggregation -embedded -config /home/vasco/w/dia-test/erlang-rts-part-2/rel/aggregation/dev/dev1/etc/app.config -args_file /home/vasco/w/dia-test/erlang-rts-part-2/rel/aggregation/dev/dev1/etc/vm.args -- console
Root: /home/vasco/w/dia-test/erlang-rts-part-2/rel/aggregation/dev/dev1
Erlang R14B03 (erts-5.8.4) [source] [rq:1] [async-threads:5] [hipe] [kernel-poll:true]</p>

<p>=INFO REPORT==== 21-Jun-2012::21:44:19 ===</p>

<pre><code>alarm_handler: {set,{{disk_almost_full,"/"},[]}}
</code></pre>

<p>** Found 0 name clashes in code paths
21:44:20.142 [info] Application lager started on node 'aggregation1@127.0.0.1'
21:44:20.236 [warning] No ring file available.
21:44:20.237 [info] Application riak_core started on node 'aggregation1@127.0.0.1'
21:44:20.249 [info] Waiting for application aggregation to start (0 seconds).
21:44:20.254 [info] Application aggregation started on node 'aggregation1@127.0.0.1'
Eshell V5.8.4  (abort with ^G)
(aggregation1@127.0.0.1)1> 21:44:20.351 [info] Wait complete for application aggregation (0 seconds)</p>

<p>```</p>

<p>We have the first node 'aggregation1@127.0.0.1' running.</p>

<p>Open a second terminal window, start the second node and join it to the first one:</p>

<p>``` bash Second console
erlang-rts-part-2/rel/aggregation $ ./dev/dev2/bin/aggregation start
erlang-rts-part-2/rel/aggregation $ ./dev/dev2/bin/aggregation-admin join aggregation1@127.0.0.1</p>

<p>Sent join request to aggregation1@127.0.0.1
```</p>

<p>Attach to the running second node and call aggregation:ping() few times. The result in my case is:</p>

<p><code>bash Second console
erlang-rts-part-2/rel/aggregation $ ./dev/dev2/bin/aggregation attach
Attaching to /tmp//home/vasco/w/dia-test/erlang-rts-part-2/rel/aggregation/dev/dev2/erlang.pipe.1 (^D to exit)
(aggregation2@127.0.0.1)1&gt; aggregation:ping().
Ping received by node : 'aggregation2@127.0.0.1'
{pong,479555224749202520035584085735030365824602865664}
(aggregation2@127.0.0.1)2&gt; aggregation:ping().
{pong,639406966332270026714112114313373821099470487552}
(aggregation2@127.0.0.1)3&gt;
</code></p>

<p>As you see, the first invocation of ping() is received by the second node: "Ping received by node : 'aggregation2@127.0.0.1'". The second ping(), however, did not print anything. If we open the first console, we'll see:</p>

<p><code>bash First console
21:45:23.394 [info] 'aggregation2@127.0.0.1' joined cluster with status 'valid'
Ping received by node : 'aggregation1@127.0.0.1'
</code></p>

<p>Hey, distributed unicorns! :)</p>

<p>Now, we can change the aggregation() command to our needs or create any other command we need to call from the Diameter server.</p>

<h2>Credits</h2>

<p>Ryan Zezeski for the Riak <a href="https://github.com/rzezeski/try-try-try">tutorial</a> and rebar <a href="https://github.com/rzezeski/rebar_riak_core">templates</a></p>

<h2>References</h2>

<p>(*) <a href="http://en.wikipedia.org/wiki/Standing_on_the_shoulders_of_giants">"Standing on the shoulders of giants"</a></p>
]]></content>
  </entry>
  
</feed>
